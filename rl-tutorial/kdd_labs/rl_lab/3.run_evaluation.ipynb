{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0f74f3d",
   "metadata": {},
   "source": [
    "# Model Evaluation: Testing GRPO-Trained Mathematical Reasoning Models\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook is designed to evaluate the performance of language models that have been trained using **Group Relative Policy Optimization (GRPO)** from the tutorial in `1. grpo_training_nemo_rl.ipynb`. After completing the GRPO training process, you can use this notebook to assess how well your model has learned to solve mathematical reasoning problems.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before running this evaluation:\n",
    "\n",
    "1. **Completed Training**: You should have successfully run the GRPO training from the first notebook. **MAKE SURE THE TRAINING NOTEBOOK IS STOPPED BEFORE RUNNING THIS EVALUATION NOTEBOOK**. If we run the two notebooks at the same time, **we would run out of GPU resources**.\n",
    "2. **Model Checkpoints**: Training should have generated model checkpoints in `results/grpo/step_X/`\n",
    "3. **System Requirements**: Sufficient GPU memory for model inference during evaluation\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a0679c",
   "metadata": {},
   "source": [
    "## Convert Checkpoint to Huggingface Format\n",
    "We first need to convert the checkpoint to Huggingface format to begin our next steps.\n",
    "Replace STEP_X with the step number of the checkpoint you want to evaluate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92ef5406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved HF checkpoint to: results/grpo_kl_0_max_seq_len_2048/step_60/hf\n"
     ]
    }
   ],
   "source": [
    "# Convert checkpoint to HuggingFace format (Replace STEP_X with the step number of the checkpoint you want to evaluate)\n",
    "\n",
    "\n",
    "!cd /root/verb-workspace/NeMo-RL && uv run python examples/convert_dcp_to_hf.py \\\n",
    "    --config results/grpo/step_X/config.yaml \\\n",
    "    --dcp-ckpt-path results/grpo/step_X/policy/weights/ \\\n",
    "    --hf-ckpt-path results/grpo/step_X/hf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e31e8cb-d804-4a53-999e-a4f2846df484",
   "metadata": {},
   "source": [
    "## Modify NeMo-RL Evaluation Code to Store Results\n",
    "\n",
    "`eval.py` in NeMo-RL does not store the evaluation responses, it only prints out the final metric. So we have to swap that `eval.py` out with a modified one that stores the results for us to see later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "073ecc10-177f-4a82-845a-bd5de719aac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace NeMo-RL's eval.py with modified version that saves the outputs\n",
    "!cp eval.py /root/verb-workspace/NeMo-RL/nemo_rl/evals/eval.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e98bec7",
   "metadata": {},
   "source": [
    "## Run Eval for Model Trained with RL\n",
    "Now let's test our trained model on MATH500:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7b3a78a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded configuration from: /root/verb-workspace/NeMo-RL/examples/configs/eval.yaml\n",
      "Overrides: {'generation': {'model_name': '/root/verb-workspace/NeMo-RL/results/grpo_kl_0_max_seq_len_2048/step_60/hf'}, 'data': {'dataset_name': 'HuggingFaceH4/MATH-500', 'dataset_key': 'test'}, 'eval': {'save_path': 'result_RL.parquet'}}\n",
      "Applied CLI overrides\n",
      "Final config:\n",
      "{'cluster': {'gpus_per_node': 1, 'num_nodes': 1},\n",
      " 'data': {'dataset_key': 'test',\n",
      "          'dataset_name': 'HuggingFaceH4/MATH-500',\n",
      "          'max_input_seq_length': 2048,\n",
      "          'problem_key': 'problem',\n",
      "          'prompt_file': None,\n",
      "          'solution_key': 'answer',\n",
      "          'system_prompt_file': None},\n",
      " 'env': {'math': {'num_workers': 8}},\n",
      " 'eval': {'metric': 'pass@1',\n",
      "          'num_tests_per_prompt': 1,\n",
      "          'save_path': 'result_RL.parquet',\n",
      "          'seed': 42},\n",
      " 'generation': {'backend': 'vllm',\n",
      "                'max_new_tokens': 2048,\n",
      "                'model_name': '/root/verb-workspace/NeMo-RL/results/grpo_kl_0_max_seq_len_2048/step_60/hf',\n",
      "                'num_prompts_per_step': -1,\n",
      "                'stop_strings': None,\n",
      "                'stop_token_ids': None,\n",
      "                'temperature': 0.0,\n",
      "                'top_k': -1,\n",
      "                'top_p': 1.0,\n",
      "                'vllm_cfg': {'gpu_memory_utilization': 0.9,\n",
      "                             'max_model_len': 2048,\n",
      "                             'precision': 'bfloat16',\n",
      "                             'tensor_parallel_size': 1}},\n",
      " 'tokenizer': {'chat_template': 'default',\n",
      "               'name': '/root/verb-workspace/NeMo-RL/results/grpo_kl_0_max_seq_len_2048/step_60/hf'}}\n",
      "WARNING:root:UV_CACHE_DIR is not set, using default cache dir\n",
      "2025-07-22 07:45:06,573\tWARNING services.py:2072 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 8589524992 bytes available. This will harm performance! You may be able to free up space by deleting files in /dev/shm. If you are inside a Docker container, you can increase /dev/shm size by passing '--shm-size=10.24gb' to 'docker run' (or add it to the run_options list in a Ray cluster config). Make sure to set this to more than 30% of available RAM.\n",
      "2025-07-22 07:45:06,710\tINFO worker.py:1841 -- Started a local Ray instance.\n",
      "INFO:nemo_rl.distributed.virtual_cluster:Started local cluster with tag 'nrl_tag_ALL': {'node:__internal_head__': 1.0, 'nrl_tag_ALL': 1.0, 'node:172.17.0.2': 1.0, 'CPU': 26.0, 'memory': 211481782272.0, 'object_store_memory': 10000000000.0, 'GPU': 1.0, 'accelerator_type:H100': 1.0}\n",
      "Using tokenizer's default chat template\n",
      "\n",
      "▶ Setting up data...\n",
      "  ✓ Evaluation dataset loaded with 500 samples\n",
      "\n",
      "▶ Setting up compute cluster...\n",
      "  ✓ Ray cluster initialized with 1 nodes\n",
      "\n",
      "▶ Setting up model...\n",
      "INFO:nemo_rl.utils.venvs:NEMO_RL_VENV_DIR is set to /root/verb-workspace/NeMo-RL/venvs.\n",
      "Using CPython \u001b[36m3.12.11\u001b[39m\n",
      "Creating virtual environment at: \u001b[36mvenvs/nemo_rl.models.generation.vllm.VllmGenerationWorker\u001b[39m\n",
      "Activate with: \u001b[32msource venvs/nemo_rl.models.generation.vllm.VllmGenerationWorker/bin/activate\u001b[39m\n",
      "\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1m`VIRTUAL_ENV=.venv` does not match the project environment path `venvs/nemo_rl.models.generation.vllm.VllmGenerationWorker` and will be ignored; use `--active` to target the active environment instead\u001b[0m\n",
      "Finished creating venv /root/verb-workspace/NeMo-RL/venvs/nemo_rl.models.generation.vllm.VllmGenerationWorker\n",
      "  ✓ Using vLLM backend for generation with /root/verb-workspace/NeMo-RL/results/grpo_kl_0_max_seq_len_2048/step_60/hf\n",
      "\n",
      "============================================================\n",
      "                  SETUP COMPLETE\n",
      "============================================================\n",
      "\n",
      "\u001b[36m(VllmGenerationWorker pid=168157)\u001b[0m INFO 07-22 07:45:12 [__init__.py:239] Automatically detected platform cuda.\n",
      "\u001b[36m(VllmGenerationWorker pid=168157)\u001b[0m INFO 07-22 07:45:18 [config.py:689] This model supports multiple tasks: {'classify', 'generate', 'embed', 'score', 'reward'}. Defaulting to 'generate'.\n",
      "\u001b[36m(VllmGenerationWorker pid=168157)\u001b[0m INFO 07-22 07:45:18 [config.py:1901] Chunked prefill is enabled with max_num_batched_tokens=16384.\n",
      "\u001b[36m(VllmGenerationWorker pid=168157)\u001b[0m WARNING 07-22 07:45:18 [cuda.py:96] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "\u001b[36m(VllmGenerationWorker pid=168157)\u001b[0m INFO 07-22 07:45:20 [core.py:61] Initializing a V1 LLM engine (v0.8.4) with config: model='/root/verb-workspace/NeMo-RL/results/grpo_kl_0_max_seq_len_2048/step_60/hf', speculative_config=None, tokenizer='/root/verb-workspace/NeMo-RL/results/grpo_kl_0_max_seq_len_2048/step_60/hf', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/root/verb-workspace/NeMo-RL/results/grpo_kl_0_max_seq_len_2048/step_60/hf, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[],\"max_capture_size\":0}\n",
      "\u001b[36m(VllmGenerationWorker pid=168157)\u001b[0m INFO 07-22 07:45:20 [worker_base.py:589] Injected <class 'nemo_rl.models.generation.vllm_backend.VllmInternalWorkerExtension'> into <class 'vllm.v1.worker.gpu_worker.Worker'> for extended collective_rpc calls ['report_device_id', 'update_weights_from_ipc_handles']\n",
      "\u001b[36m(VllmGenerationWorker pid=168157)\u001b[0m WARNING 07-22 07:45:20 [utils.py:2444] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x70790cd79c70>\n",
      "\u001b[36m(VllmGenerationWorker pid=168157)\u001b[0m INFO 07-22 07:45:20 [parallel_state.py:959] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "\u001b[36m(VllmGenerationWorker pid=168157)\u001b[0m INFO 07-22 07:45:20 [cuda.py:221] Using Flash Attention backend on V1 engine.\n",
      "\u001b[36m(VllmGenerationWorker pid=168157)\u001b[0m INFO 07-22 07:45:20 [gpu_model_runner.py:1276] Starting to load model /root/verb-workspace/NeMo-RL/results/grpo_kl_0_max_seq_len_2048/step_60/hf...\n",
      "Loading pt checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "\u001b[36m(VllmGenerationWorker pid=168157)\u001b[0m WARNING 07-22 07:45:20 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "Loading pt checkpoint shards: 100% Completed | 1/1 [00:05<00:00,  5.52s/it]\n",
      "Loading pt checkpoint shards: 100% Completed | 1/1 [00:05<00:00,  5.52s/it]\n",
      "\u001b[36m(VllmGenerationWorker pid=168157)\u001b[0m \n",
      "\u001b[36m(VllmGenerationWorker pid=168157)\u001b[0m INFO 07-22 07:45:26 [loader.py:458] Loading weights took 5.55 seconds\n",
      "\u001b[36m(VllmGenerationWorker pid=168157)\u001b[0m INFO 07-22 07:45:26 [gpu_model_runner.py:1291] Model loading took 2.9105 GiB and 5.643538 seconds\n",
      "\u001b[36m(VllmGenerationWorker pid=168157)\u001b[0m INFO 07-22 07:45:26 [kv_cache_utils.py:634] GPU KV cache size: 2,320,656 tokens\n",
      "\u001b[36m(VllmGenerationWorker pid=168157)\u001b[0m INFO 07-22 07:45:26 [kv_cache_utils.py:637] Maximum concurrency for 2,048 tokens per request: 1133.13x\n",
      "\u001b[36m(VllmGenerationWorker pid=168157)\u001b[0m INFO 07-22 07:45:26 [core.py:163] init engine (profile, create kv cache, warmup model) took 0.42 seconds\n",
      "Processed prompts:   0%|          | 0/500 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\n",
      "Processed prompts:   0%|          | 1/500 [00:00<05:07,  1.62it/s, est. speed input: 51.92 toks/s, output: 25.96 toks/s]\n",
      "Processed prompts:   0%|          | 2/500 [00:00<03:47,  2.19it/s, est. speed input: 73.73 toks/s, output: 61.27 toks/s]\n",
      "Processed prompts:   1%|          | 3/500 [00:01<04:02,  2.05it/s, est. speed input: 69.97 toks/s, output: 92.17 toks/s]\n",
      "Processed prompts:   1%|          | 6/500 [00:02<02:45,  2.98it/s, est. speed input: 103.63 toks/s, output: 192.20 toks/s]\n",
      "Processed prompts:   1%|▏         | 7/500 [00:02<02:45,  2.98it/s, est. speed input: 100.58 toks/s, output: 227.76 toks/s]\n",
      "Processed prompts:   2%|▏         | 9/500 [00:02<01:48,  4.54it/s, est. speed input: 146.35 toks/s, output: 339.37 toks/s]\n",
      "Processed prompts:   2%|▏         | 11/500 [00:02<01:22,  5.91it/s, est. speed input: 177.20 toks/s, output: 438.62 toks/s]\n",
      "Processed prompts:   3%|▎         | 13/500 [00:02<01:03,  7.61it/s, est. speed input: 197.68 toks/s, output: 541.02 toks/s]\n",
      "Processed prompts:   3%|▎         | 16/500 [00:03<00:47, 10.13it/s, est. speed input: 240.55 toks/s, output: 692.39 toks/s]\n",
      "Processed prompts:   4%|▍         | 22/500 [00:03<00:27, 17.56it/s, est. speed input: 336.20 toks/s, output: 1025.67 toks/s]\n",
      "Processed prompts:   5%|▌         | 26/500 [00:03<00:23, 19.92it/s, est. speed input: 409.38 toks/s, output: 1224.05 toks/s]\n",
      "Processed prompts:   7%|▋         | 33/500 [00:03<00:15, 29.36it/s, est. speed input: 496.37 toks/s, output: 1613.04 toks/s]\n",
      "Processed prompts:   8%|▊         | 40/500 [00:03<00:12, 36.56it/s, est. speed input: 639.30 toks/s, output: 1987.68 toks/s]\n",
      "Processed prompts:  10%|█         | 51/500 [00:03<00:08, 52.41it/s, est. speed input: 843.20 toks/s, output: 2605.35 toks/s]\n",
      "Processed prompts:  12%|█▏        | 58/500 [00:03<00:09, 47.51it/s, est. speed input: 958.54 toks/s, output: 2915.97 toks/s]\n",
      "Processed prompts:  13%|█▎        | 64/500 [00:04<00:08, 49.89it/s, est. speed input: 998.22 toks/s, output: 3213.17 toks/s]\n",
      "Processed prompts:  14%|█▍        | 70/500 [00:04<00:08, 50.08it/s, est. speed input: 1090.59 toks/s, output: 3493.43 toks/s]\n",
      "Processed prompts:  15%|█▌        | 76/500 [00:04<00:08, 52.10it/s, est. speed input: 1178.74 toks/s, output: 3780.84 toks/s]\n",
      "Processed prompts:  16%|█▋        | 82/500 [00:04<00:07, 53.65it/s, est. speed input: 1231.33 toks/s, output: 4064.85 toks/s]\n",
      "Processed prompts:  18%|█▊        | 88/500 [00:04<00:08, 45.99it/s, est. speed input: 1269.32 toks/s, output: 4281.05 toks/s]\n",
      "Processed prompts:  19%|█▉        | 95/500 [00:04<00:07, 51.47it/s, est. speed input: 1324.73 toks/s, output: 4623.53 toks/s]\n",
      "Processed prompts:  20%|██        | 101/500 [00:04<00:09, 43.57it/s, est. speed input: 1365.59 toks/s, output: 4816.65 toks/s]\n",
      "Processed prompts:  22%|██▏       | 111/500 [00:04<00:06, 55.81it/s, est. speed input: 1443.49 toks/s, output: 5343.02 toks/s]\n",
      "Processed prompts:  24%|██▍       | 120/500 [00:05<00:05, 63.41it/s, est. speed input: 1524.03 toks/s, output: 5797.07 toks/s]\n",
      "Processed prompts:  26%|██▌       | 129/500 [00:05<00:05, 67.56it/s, est. speed input: 1648.15 toks/s, output: 6235.95 toks/s]\n",
      "Processed prompts:  27%|██▋       | 137/500 [00:05<00:06, 57.99it/s, est. speed input: 1711.41 toks/s, output: 6524.88 toks/s]\n",
      "Processed prompts:  29%|██▉       | 144/500 [00:05<00:06, 56.99it/s, est. speed input: 1734.77 toks/s, output: 6814.39 toks/s]\n",
      "Processed prompts:  31%|███       | 153/500 [00:05<00:05, 62.80it/s, est. speed input: 1789.96 toks/s, output: 7246.37 toks/s]\n",
      "Processed prompts:  32%|███▏      | 160/500 [00:05<00:05, 58.50it/s, est. speed input: 1823.44 toks/s, output: 7513.81 toks/s]\n",
      "Processed prompts:  34%|███▍      | 170/500 [00:05<00:04, 66.50it/s, est. speed input: 1960.44 toks/s, output: 8007.47 toks/s]\n",
      "Processed prompts:  36%|███▌      | 179/500 [00:05<00:04, 68.02it/s, est. speed input: 2022.84 toks/s, output: 8413.29 toks/s]\n",
      "Processed prompts:  37%|███▋      | 187/500 [00:06<00:05, 62.23it/s, est. speed input: 2076.08 toks/s, output: 8711.70 toks/s]\n",
      "Processed prompts:  39%|███▉      | 194/500 [00:06<00:05, 53.60it/s, est. speed input: 2212.37 toks/s, output: 8906.65 toks/s]\n",
      "Processed prompts:  41%|████      | 203/500 [00:06<00:04, 60.10it/s, est. speed input: 2389.26 toks/s, output: 9332.80 toks/s]\n",
      "Processed prompts:  44%|████▍     | 219/500 [00:06<00:03, 81.55it/s, est. speed input: 2569.39 toks/s, output: 10204.84 toks/s]\n",
      "Processed prompts:  46%|████▌     | 231/500 [00:06<00:03, 89.03it/s, est. speed input: 2694.35 toks/s, output: 10814.42 toks/s]\n",
      "Processed prompts:  48%|████▊     | 241/500 [00:06<00:03, 73.23it/s, est. speed input: 2813.18 toks/s, output: 11143.37 toks/s]\n",
      "Processed prompts:  50%|█████     | 250/500 [00:07<00:04, 57.66it/s, est. speed input: 2852.15 toks/s, output: 11336.23 toks/s]\n",
      "Processed prompts:  51%|█████▏    | 257/500 [00:07<00:04, 51.82it/s, est. speed input: 2841.98 toks/s, output: 11512.00 toks/s]\n",
      "Processed prompts:  53%|█████▎    | 263/500 [00:07<00:04, 53.17it/s, est. speed input: 2890.69 toks/s, output: 11749.48 toks/s]\n",
      "Processed prompts:  54%|█████▍    | 270/500 [00:07<00:04, 51.11it/s, est. speed input: 2927.09 toks/s, output: 11972.64 toks/s]\n",
      "Processed prompts:  56%|█████▌    | 278/500 [00:07<00:03, 57.21it/s, est. speed input: 2998.94 toks/s, output: 12344.20 toks/s]\n",
      "Processed prompts:  57%|█████▋    | 285/500 [00:07<00:04, 52.48it/s, est. speed input: 3040.24 toks/s, output: 12551.06 toks/s]\n",
      "Processed prompts:  59%|█████▉    | 294/500 [00:07<00:03, 59.30it/s, est. speed input: 3093.32 toks/s, output: 12972.70 toks/s]\n",
      "Processed prompts:  60%|██████    | 301/500 [00:08<00:03, 54.12it/s, est. speed input: 3105.99 toks/s, output: 13184.25 toks/s]\n",
      "Processed prompts:  61%|██████▏   | 307/500 [00:08<00:03, 52.61it/s, est. speed input: 3160.45 toks/s, output: 13388.46 toks/s]\n",
      "Processed prompts:  63%|██████▎   | 316/500 [00:08<00:03, 58.36it/s, est. speed input: 3207.57 toks/s, output: 13798.17 toks/s]\n",
      "Processed prompts:  65%|██████▍   | 323/500 [00:08<00:04, 42.21it/s, est. speed input: 3162.02 toks/s, output: 13812.86 toks/s]\n",
      "Processed prompts:  66%|██████▌   | 329/500 [00:08<00:03, 43.91it/s, est. speed input: 3241.79 toks/s, output: 14032.40 toks/s]\n",
      "Processed prompts:  67%|██████▋   | 334/500 [00:08<00:03, 41.57it/s, est. speed input: 3238.02 toks/s, output: 14149.70 toks/s]\n",
      "Processed prompts:  68%|██████▊   | 341/500 [00:08<00:03, 46.02it/s, est. speed input: 3268.06 toks/s, output: 14444.61 toks/s]\n",
      "Processed prompts:  69%|██████▉   | 346/500 [00:09<00:03, 46.35it/s, est. speed input: 3285.51 toks/s, output: 14621.27 toks/s]\n",
      "Processed prompts:  71%|███████   | 353/500 [00:09<00:02, 51.60it/s, est. speed input: 3309.40 toks/s, output: 14938.54 toks/s]\n",
      "Processed prompts:  72%|███████▏  | 359/500 [00:09<00:02, 50.46it/s, est. speed input: 3315.31 toks/s, output: 15152.02 toks/s]\n",
      "Processed prompts:  73%|███████▎  | 365/500 [00:09<00:02, 51.20it/s, est. speed input: 3344.31 toks/s, output: 15388.45 toks/s]\n",
      "Processed prompts:  74%|███████▍  | 372/500 [00:09<00:02, 52.96it/s, est. speed input: 3361.44 toks/s, output: 15678.49 toks/s]\n",
      "Processed prompts:  76%|███████▌  | 378/500 [00:09<00:02, 42.56it/s, est. speed input: 3339.10 toks/s, output: 15761.34 toks/s]\n",
      "Processed prompts:  77%|███████▋  | 383/500 [00:09<00:02, 43.36it/s, est. speed input: 3363.26 toks/s, output: 15941.13 toks/s]\n",
      "Processed prompts:  78%|███████▊  | 388/500 [00:10<00:03, 35.99it/s, est. speed input: 3362.84 toks/s, output: 15970.56 toks/s]\n",
      "Processed prompts:  79%|███████▊  | 393/500 [00:10<00:03, 31.47it/s, est. speed input: 3336.66 toks/s, output: 15996.16 toks/s]\n",
      "Processed prompts:  80%|███████▉  | 399/500 [00:10<00:03, 32.16it/s, est. speed input: 3337.57 toks/s, output: 16150.42 toks/s]\n",
      "Processed prompts:  81%|████████  | 404/500 [00:10<00:02, 35.27it/s, est. speed input: 3354.98 toks/s, output: 16349.09 toks/s]\n",
      "Processed prompts:  82%|████████▏ | 408/500 [00:10<00:02, 32.57it/s, est. speed input: 3343.33 toks/s, output: 16405.72 toks/s]\n",
      "Processed prompts:  82%|████████▏ | 412/500 [00:10<00:03, 27.86it/s, est. speed input: 3313.75 toks/s, output: 16388.75 toks/s]\n",
      "Processed prompts:  83%|████████▎ | 416/500 [00:11<00:04, 20.94it/s, est. speed input: 3270.00 toks/s, output: 16211.70 toks/s]\n",
      "Processed prompts:  84%|████████▍ | 421/500 [00:11<00:03, 24.18it/s, est. speed input: 3266.02 toks/s, output: 16380.79 toks/s]\n",
      "Processed prompts:  85%|████████▌ | 425/500 [00:11<00:02, 26.14it/s, est. speed input: 3255.49 toks/s, output: 16507.89 toks/s]\n",
      "Processed prompts:  86%|████████▋ | 432/500 [00:11<00:02, 33.63it/s, est. speed input: 3261.58 toks/s, output: 16859.61 toks/s]\n",
      "Processed prompts:  87%|████████▋ | 436/500 [00:11<00:02, 31.25it/s, est. speed input: 3264.42 toks/s, output: 16935.81 toks/s]\n",
      "Processed prompts:  88%|████████▊ | 441/500 [00:11<00:02, 28.80it/s, est. speed input: 3246.05 toks/s, output: 17019.76 toks/s]\n",
      "Processed prompts:  89%|████████▉ | 445/500 [00:12<00:02, 23.85it/s, est. speed input: 3218.18 toks/s, output: 16974.00 toks/s]\n",
      "Processed prompts:  90%|████████▉ | 448/500 [00:12<00:02, 20.23it/s, est. speed input: 3174.39 toks/s, output: 16890.46 toks/s]\n",
      "Processed prompts:  91%|█████████ | 454/500 [00:12<00:01, 27.11it/s, est. speed input: 3186.78 toks/s, output: 17215.34 toks/s]\n",
      "Processed prompts:  92%|█████████▏| 458/500 [00:12<00:01, 28.79it/s, est. speed input: 3195.02 toks/s, output: 17366.95 toks/s]\n",
      "Processed prompts:  92%|█████████▏| 462/500 [00:12<00:01, 25.73it/s, est. speed input: 3175.30 toks/s, output: 17407.32 toks/s]\n",
      "Processed prompts:  93%|█████████▎| 466/500 [00:12<00:01, 27.84it/s, est. speed input: 3176.19 toks/s, output: 17565.54 toks/s]\n",
      "Processed prompts:  94%|█████████▍| 470/500 [00:13<00:01, 22.24it/s, est. speed input: 3162.37 toks/s, output: 17520.83 toks/s]\n",
      "Processed prompts:  95%|█████████▍| 473/500 [00:13<00:02, 13.30it/s, est. speed input: 3072.36 toks/s, output: 17092.53 toks/s]\n",
      "Processed prompts:  95%|█████████▌| 476/500 [00:13<00:01, 14.94it/s, est. speed input: 3075.80 toks/s, output: 17174.86 toks/s]\n",
      "Processed prompts:  96%|█████████▌| 479/500 [00:14<00:01, 16.13it/s, est. speed input: 3068.74 toks/s, output: 17238.21 toks/s]\n",
      "Processed prompts:  96%|█████████▋| 482/500 [00:15<00:02,  7.29it/s, est. speed input: 2880.01 toks/s, output: 16330.30 toks/s]\n",
      "Processed prompts:  97%|█████████▋| 484/500 [00:15<00:03,  5.20it/s, est. speed input: 2745.64 toks/s, output: 15665.96 toks/s]\n",
      "Processed prompts:  97%|█████████▋| 486/500 [00:16<00:03,  4.09it/s, est. speed input: 2616.35 toks/s, output: 15045.89 toks/s]\n",
      "Processed prompts:  97%|█████████▋| 487/500 [00:17<00:03,  3.35it/s, est. speed input: 2528.46 toks/s, output: 14584.37 toks/s]\n",
      "Processed prompts:  98%|█████████▊| 488/500 [00:17<00:03,  3.68it/s, est. speed input: 2514.03 toks/s, output: 14555.50 toks/s]\n",
      "Processed prompts:  98%|█████████▊| 489/500 [00:18<00:03,  3.00it/s, est. speed input: 2440.41 toks/s, output: 14166.00 toks/s]\n",
      "Processed prompts:  98%|█████████▊| 490/500 [00:18<00:03,  3.32it/s, est. speed input: 2420.32 toks/s, output: 14113.76 toks/s]\n",
      "Processed prompts:  98%|█████████▊| 491/500 [00:19<00:03,  2.39it/s, est. speed input: 2321.52 toks/s, output: 13610.20 toks/s]\n",
      "Processed prompts:  98%|█████████▊| 492/500 [00:20<00:05,  1.49it/s, est. speed input: 2168.27 toks/s, output: 12753.12 toks/s]\n",
      "Processed prompts:  99%|█████████▉| 494/500 [00:21<00:03,  1.83it/s, est. speed input: 2104.73 toks/s, output: 12489.78 toks/s]\n",
      "Processed prompts:  99%|█████████▉| 495/500 [00:21<00:02,  2.23it/s, est. speed input: 2095.74 toks/s, output: 12508.70 toks/s]\n",
      "Processed prompts:  99%|█████████▉| 497/500 [00:21<00:00,  3.29it/s, est. speed input: 2085.23 toks/s, output: 12596.54 toks/s]\n",
      "Processed prompts: 100%|██████████| 500/500 [00:21<00:00, 23.12it/s, est. speed input: 2079.86 toks/s, output: 12806.27 toks/s]\n",
      "\n",
      "✓ Evaluation data saved to: result_RL.parquet\n",
      "  Total samples: 500\n",
      "  Columns: ['prompt', 'response', 'reward', 'message_log', 'extra_env_info', 'sample_index', 'model_name', 'dataset_name', 'metric', 'num_tests_per_prompt', 'temperature', 'top_p', 'top_k']\n",
      "  File size: 0.85 MB\n",
      "\n",
      "============================================================\n",
      "model_name='hf' dataset_name='MATH-500'\n",
      "max_new_tokens=2048 temperature=0.0 top_p=1.0 top_k=-1\n",
      "\n",
      "metric='pass@1' num_tests_per_prompt=1\n",
      "\n",
      "score=0.5360 (268.0/500)\n",
      "============================================================\n",
      "\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Run evaluation for model trained with RL (Replace STEP_X with the step number of the checkpoint you want to evaluate)\n",
    "\n",
    "!cd /root/verb-workspace/NeMo-RL && uv run python examples/run_eval.py \\\n",
    "    generation.model_name=$PWD/results/grpo/step_X/hf \\\n",
    "    data.dataset_name=HuggingFaceH4/MATH-500 \\\n",
    "    data.dataset_key=test \\\n",
    "    eval.save_path=result_RL.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a7de7d",
   "metadata": {},
   "source": [
    "## Run Eval for Base Model\n",
    "We then test the base model on MATH500 for a comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8f2c4826-06df-4b77-bf1a-0ef981de50e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded configuration from: /root/verb-workspace/NeMo-RL/examples/configs/eval.yaml\n",
      "Overrides: {'generation': {'model_name': 'Qwen/Qwen2.5-1.5B'}, 'data': {'dataset_name': 'HuggingFaceH4/MATH-500', 'dataset_key': 'test'}, 'eval': {'save_path': 'result_Base.parquet'}}\n",
      "Applied CLI overrides\n",
      "Final config:\n",
      "{'cluster': {'gpus_per_node': 1, 'num_nodes': 1},\n",
      " 'data': {'dataset_key': 'test',\n",
      "          'dataset_name': 'HuggingFaceH4/MATH-500',\n",
      "          'max_input_seq_length': 2048,\n",
      "          'problem_key': 'problem',\n",
      "          'prompt_file': None,\n",
      "          'solution_key': 'answer',\n",
      "          'system_prompt_file': None},\n",
      " 'env': {'math': {'num_workers': 8}},\n",
      " 'eval': {'metric': 'pass@1',\n",
      "          'num_tests_per_prompt': 1,\n",
      "          'save_path': 'result_Base.parquet',\n",
      "          'seed': 42},\n",
      " 'generation': {'backend': 'vllm',\n",
      "                'max_new_tokens': 2048,\n",
      "                'model_name': 'Qwen/Qwen2.5-1.5B',\n",
      "                'num_prompts_per_step': -1,\n",
      "                'stop_strings': None,\n",
      "                'stop_token_ids': None,\n",
      "                'temperature': 0.0,\n",
      "                'top_k': -1,\n",
      "                'top_p': 1.0,\n",
      "                'vllm_cfg': {'gpu_memory_utilization': 0.9,\n",
      "                             'max_model_len': 2048,\n",
      "                             'precision': 'bfloat16',\n",
      "                             'tensor_parallel_size': 1}},\n",
      " 'tokenizer': {'chat_template': 'default', 'name': 'Qwen/Qwen2.5-1.5B'}}\n",
      "WARNING:root:UV_CACHE_DIR is not set, using default cache dir\n",
      "2025-07-22 07:48:16,634\tWARNING services.py:2072 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 8589524992 bytes available. This will harm performance! You may be able to free up space by deleting files in /dev/shm. If you are inside a Docker container, you can increase /dev/shm size by passing '--shm-size=10.24gb' to 'docker run' (or add it to the run_options list in a Ray cluster config). Make sure to set this to more than 30% of available RAM.\n",
      "2025-07-22 07:48:16,772\tINFO worker.py:1841 -- Started a local Ray instance.\n",
      "INFO:nemo_rl.distributed.virtual_cluster:Started local cluster with tag 'nrl_tag_ALL': {'nrl_tag_ALL': 1.0, 'node:__internal_head__': 1.0, 'node:172.17.0.2': 1.0, 'CPU': 26.0, 'object_store_memory': 10000000000.0, 'memory': 211481147392.0, 'GPU': 1.0, 'accelerator_type:H100': 1.0}\n",
      "Using tokenizer's default chat template\n",
      "\n",
      "▶ Setting up data...\n",
      "  ✓ Evaluation dataset loaded with 500 samples\n",
      "\n",
      "▶ Setting up compute cluster...\n",
      "  ✓ Ray cluster initialized with 1 nodes\n",
      "\n",
      "▶ Setting up model...\n",
      "INFO:nemo_rl.utils.venvs:NEMO_RL_VENV_DIR is set to /root/verb-workspace/NeMo-RL/venvs.\n",
      "Using CPython \u001b[36m3.12.11\u001b[39m\n",
      "Creating virtual environment at: \u001b[36mvenvs/nemo_rl.models.generation.vllm.VllmGenerationWorker\u001b[39m\n",
      "Activate with: \u001b[32msource venvs/nemo_rl.models.generation.vllm.VllmGenerationWorker/bin/activate\u001b[39m\n",
      "\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1m`VIRTUAL_ENV=.venv` does not match the project environment path `venvs/nemo_rl.models.generation.vllm.VllmGenerationWorker` and will be ignored; use `--active` to target the active environment instead\u001b[0m\n",
      "Finished creating venv /root/verb-workspace/NeMo-RL/venvs/nemo_rl.models.generation.vllm.VllmGenerationWorker\n",
      "  ✓ Using vLLM backend for generation with Qwen/Qwen2.5-1.5B\n",
      "\n",
      "============================================================\n",
      "                  SETUP COMPLETE\n",
      "============================================================\n",
      "\n",
      "\u001b[36m(VllmGenerationWorker pid=171873)\u001b[0m INFO 07-22 07:48:22 [__init__.py:239] Automatically detected platform cuda.\n",
      "\u001b[36m(VllmGenerationWorker pid=171873)\u001b[0m INFO 07-22 07:48:29 [config.py:689] This model supports multiple tasks: {'score', 'generate', 'classify', 'reward', 'embed'}. Defaulting to 'generate'.\n",
      "\u001b[36m(VllmGenerationWorker pid=171873)\u001b[0m INFO 07-22 07:48:29 [config.py:1901] Chunked prefill is enabled with max_num_batched_tokens=16384.\n",
      "\u001b[36m(VllmGenerationWorker pid=171873)\u001b[0m WARNING 07-22 07:48:29 [cuda.py:96] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "\u001b[36m(VllmGenerationWorker pid=171873)\u001b[0m INFO 07-22 07:48:30 [core.py:61] Initializing a V1 LLM engine (v0.8.4) with config: model='Qwen/Qwen2.5-1.5B', speculative_config=None, tokenizer='Qwen/Qwen2.5-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=Qwen/Qwen2.5-1.5B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[],\"max_capture_size\":0}\n",
      "\u001b[36m(VllmGenerationWorker pid=171873)\u001b[0m INFO 07-22 07:48:30 [worker_base.py:589] Injected <class 'nemo_rl.models.generation.vllm_backend.VllmInternalWorkerExtension'> into <class 'vllm.v1.worker.gpu_worker.Worker'> for extended collective_rpc calls ['report_device_id', 'update_weights_from_ipc_handles']\n",
      "\u001b[36m(VllmGenerationWorker pid=171873)\u001b[0m WARNING 07-22 07:48:30 [utils.py:2444] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7c14e57ff500>\n",
      "\u001b[36m(VllmGenerationWorker pid=171873)\u001b[0m INFO 07-22 07:48:31 [parallel_state.py:959] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "\u001b[36m(VllmGenerationWorker pid=171873)\u001b[0m INFO 07-22 07:48:31 [cuda.py:221] Using Flash Attention backend on V1 engine.\n",
      "\u001b[36m(VllmGenerationWorker pid=171873)\u001b[0m INFO 07-22 07:48:31 [gpu_model_runner.py:1276] Starting to load model Qwen/Qwen2.5-1.5B...\n",
      "\u001b[36m(VllmGenerationWorker pid=171873)\u001b[0m WARNING 07-22 07:48:31 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[36m(VllmGenerationWorker pid=171873)\u001b[0m INFO 07-22 07:48:31 [weight_utils.py:265] Using model weights format ['*.safetensors']\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "\u001b[36m(VllmGenerationWorker pid=171873)\u001b[0m INFO 07-22 07:48:31 [weight_utils.py:315] No model.safetensors.index.json found in remote.\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.12it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.12it/s]\n",
      "\u001b[36m(VllmGenerationWorker pid=171873)\u001b[0m \n",
      "\u001b[36m(VllmGenerationWorker pid=171873)\u001b[0m INFO 07-22 07:48:31 [loader.py:458] Loading weights took 0.51 seconds\n",
      "\u001b[36m(VllmGenerationWorker pid=171873)\u001b[0m INFO 07-22 07:48:32 [gpu_model_runner.py:1291] Model loading took 2.9105 GiB and 0.835572 seconds\n",
      "\u001b[36m(VllmGenerationWorker pid=171873)\u001b[0m INFO 07-22 07:48:32 [kv_cache_utils.py:634] GPU KV cache size: 2,320,656 tokens\n",
      "\u001b[36m(VllmGenerationWorker pid=171873)\u001b[0m INFO 07-22 07:48:32 [kv_cache_utils.py:637] Maximum concurrency for 2,048 tokens per request: 1133.13x\n",
      "\u001b[36m(VllmGenerationWorker pid=171873)\u001b[0m INFO 07-22 07:48:32 [core.py:163] init engine (profile, create kv cache, warmup model) took 0.55 seconds\n",
      "Processed prompts:   0%|          | 0/500 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\n",
      "Processed prompts:   0%|          | 1/500 [00:00<05:03,  1.64it/s, est. speed input: 52.56 toks/s, output: 23.00 toks/s]\n",
      "Processed prompts:   1%|          | 3/500 [00:00<01:53,  4.36it/s, est. speed input: 193.39 toks/s, output: 78.60 toks/s]\n",
      "Processed prompts:   1%|          | 4/500 [00:01<02:04,  3.98it/s, est. speed input: 200.08 toks/s, output: 104.15 toks/s]\n",
      "Processed prompts:   1%|          | 5/500 [00:01<02:44,  3.01it/s, est. speed input: 203.06 toks/s, output: 125.73 toks/s]\n",
      "Processed prompts:   2%|▏         | 9/500 [00:01<01:08,  7.12it/s, est. speed input: 400.85 toks/s, output: 317.81 toks/s]\n",
      "Processed prompts:   2%|▏         | 11/500 [00:02<02:15,  3.60it/s, est. speed input: 358.67 toks/s, output: 296.00 toks/s]\n",
      "Processed prompts:   2%|▏         | 12/500 [00:03<02:22,  3.41it/s, est. speed input: 330.58 toks/s, output: 325.04 toks/s]\n",
      "Processed prompts:   3%|▎         | 15/500 [00:03<01:26,  5.60it/s, est. speed input: 451.08 toks/s, output: 498.82 toks/s]\n",
      "Processed prompts:   3%|▎         | 17/500 [00:03<01:26,  5.56it/s, est. speed input: 445.44 toks/s, output: 569.71 toks/s]\n",
      "Processed prompts:   4%|▍         | 19/500 [00:04<01:59,  4.03it/s, est. speed input: 388.50 toks/s, output: 592.99 toks/s]\n",
      "Processed prompts:   4%|▍         | 21/500 [00:04<01:44,  4.58it/s, est. speed input: 381.27 toks/s, output: 681.87 toks/s]\n",
      "Processed prompts:   4%|▍         | 22/500 [00:06<04:08,  1.92it/s, est. speed input: 283.58 toks/s, output: 551.65 toks/s]\n",
      "Processed prompts:   5%|▍         | 23/500 [00:07<04:11,  1.89it/s, est. speed input: 280.78 toks/s, output: 574.25 toks/s]\n",
      "Processed prompts:   5%|▍         | 24/500 [00:11<11:08,  1.40s/it, est. speed input: 182.44 toks/s, output: 419.83 toks/s]\n",
      "Processed prompts:   5%|▌         | 25/500 [00:21<27:07,  3.43s/it, est. speed input: 137.25 toks/s, output: 286.83 toks/s]\n",
      "Processed prompts:   5%|▌         | 26/500 [00:23<24:15,  3.07s/it, est. speed input: 155.84 toks/s, output: 318.32 toks/s]\n",
      "Processed prompts:   5%|▌         | 27/500 [00:24<19:17,  2.45s/it, est. speed input: 179.25 toks/s, output: 364.53 toks/s]\n",
      "Processed prompts:   6%|▌         | 28/500 [00:29<24:55,  3.17s/it, est. speed input: 164.73 toks/s, output: 355.24 toks/s]\n",
      "Processed prompts:   6%|▌         | 29/500 [00:32<24:25,  3.11s/it, est. speed input: 160.96 toks/s, output: 374.57 toks/s]\n",
      "Processed prompts:   6%|▌         | 30/500 [00:32<17:46,  2.27s/it, est. speed input: 171.11 toks/s, output: 424.19 toks/s]\n",
      "Processed prompts:   6%|▋         | 32/500 [00:33<10:57,  1.40s/it, est. speed input: 188.26 toks/s, output: 517.46 toks/s]\n",
      "Processed prompts:   7%|▋         | 33/500 [00:34<10:20,  1.33s/it, est. speed input: 190.94 toks/s, output: 552.01 toks/s]\n",
      "Processed prompts:   7%|▋         | 34/500 [00:35<09:04,  1.17s/it, est. speed input: 194.78 toks/s, output: 591.54 toks/s]\n",
      "Processed prompts:   7%|▋         | 36/500 [00:35<05:28,  1.41it/s, est. speed input: 208.95 toks/s, output: 689.75 toks/s]\n",
      "Processed prompts:   8%|▊         | 38/500 [00:35<03:46,  2.04it/s, est. speed input: 222.07 toks/s, output: 785.43 toks/s]\n",
      "Processed prompts:   8%|▊         | 39/500 [00:35<03:08,  2.44it/s, est. speed input: 228.48 toks/s, output: 833.37 toks/s]\n",
      "Processed prompts:   8%|▊         | 41/500 [00:35<02:11,  3.49it/s, est. speed input: 241.29 toks/s, output: 930.00 toks/s]\n",
      "Processed prompts:   9%|▉         | 45/500 [00:35<01:12,  6.25it/s, est. speed input: 267.15 toks/s, output: 1126.42 toks/s]\n",
      "Processed prompts:   9%|▉         | 47/500 [00:36<01:19,  5.67it/s, est. speed input: 276.36 toks/s, output: 1212.86 toks/s]\n",
      "Processed prompts:  10%|█         | 50/500 [00:36<00:57,  7.76it/s, est. speed input: 293.41 toks/s, output: 1358.21 toks/s]\n",
      "Processed prompts:  10%|█         | 52/500 [00:36<00:50,  8.79it/s, est. speed input: 304.03 toks/s, output: 1452.91 toks/s]\n",
      "Processed prompts:  11%|█         | 54/500 [00:36<00:53,  8.31it/s, est. speed input: 312.99 toks/s, output: 1541.63 toks/s]\n",
      "Processed prompts:  11%|█         | 56/500 [00:37<00:48,  9.11it/s, est. speed input: 322.24 toks/s, output: 1634.51 toks/s]\n",
      "Processed prompts:  12%|█▏        | 59/500 [00:37<00:36, 12.13it/s, est. speed input: 337.03 toks/s, output: 1778.95 toks/s]\n",
      "Processed prompts:  12%|█▏        | 62/500 [00:37<00:43, 10.18it/s, est. speed input: 348.69 toks/s, output: 1909.07 toks/s]\n",
      "Processed prompts:  13%|█▎        | 64/500 [00:37<00:38, 11.45it/s, est. speed input: 357.07 toks/s, output: 2002.76 toks/s]\n",
      "Processed prompts:  13%|█▎        | 67/500 [00:37<00:32, 13.19it/s, est. speed input: 369.16 toks/s, output: 2142.69 toks/s]\n",
      "Processed prompts:  14%|█▍        | 70/500 [00:38<00:29, 14.50it/s, est. speed input: 380.64 toks/s, output: 2281.76 toks/s]\n",
      "Processed prompts:  15%|█▍        | 73/500 [00:38<00:29, 14.31it/s, est. speed input: 390.96 toks/s, output: 2417.00 toks/s]\n",
      "Processed prompts:  15%|█▌        | 75/500 [00:38<00:34, 12.21it/s, est. speed input: 396.27 toks/s, output: 2500.39 toks/s]\n",
      "Processed prompts:  16%|█▌        | 78/500 [00:38<00:28, 15.02it/s, est. speed input: 406.39 toks/s, output: 2641.25 toks/s]\n",
      "Processed prompts:  16%|█▌        | 80/500 [00:38<00:29, 14.21it/s, est. speed input: 411.81 toks/s, output: 2728.52 toks/s]\n",
      "Processed prompts:  17%|█▋        | 84/500 [00:38<00:21, 18.96it/s, est. speed input: 424.49 toks/s, output: 2917.69 toks/s]\n",
      "Processed prompts:  18%|█▊        | 88/500 [00:39<00:18, 21.85it/s, est. speed input: 436.32 toks/s, output: 3104.18 toks/s]\n",
      "Processed prompts:  19%|█▉        | 96/500 [00:39<00:12, 32.41it/s, est. speed input: 460.50 toks/s, output: 3486.32 toks/s]\n",
      "Processed prompts:  21%|██        | 103/500 [00:39<00:10, 38.28it/s, est. speed input: 480.52 toks/s, output: 3817.99 toks/s]\n",
      "Processed prompts:  22%|██▏       | 109/500 [00:39<00:09, 40.46it/s, est. speed input: 496.48 toks/s, output: 4099.49 toks/s]\n",
      "Processed prompts:  24%|██▍       | 121/500 [00:39<00:07, 49.40it/s, est. speed input: 526.75 toks/s, output: 4665.96 toks/s]\n",
      "Processed prompts:  26%|██▌       | 130/500 [00:39<00:06, 56.43it/s, est. speed input: 548.14 toks/s, output: 5091.96 toks/s]\n",
      "Processed prompts:  30%|██▉       | 149/500 [00:39<00:04, 86.30it/s, est. speed input: 593.05 toks/s, output: 6005.63 toks/s]\n",
      "Processed prompts:  33%|███▎      | 167/500 [00:39<00:03, 105.21it/s, est. speed input: 632.65 toks/s, output: 6867.98 toks/s]\n",
      "Processed prompts:  36%|███▋      | 182/500 [00:40<00:02, 112.80it/s, est. speed input: 663.57 toks/s, output: 7581.72 toks/s]\n",
      "Processed prompts:  42%|████▏     | 208/500 [00:40<00:01, 146.98it/s, est. speed input: 715.20 toks/s, output: 8831.02 toks/s]\n",
      "Processed prompts:  45%|████▌     | 225/500 [00:40<00:01, 150.70it/s, est. speed input: 746.05 toks/s, output: 9638.45 toks/s]\n",
      "Processed prompts:  53%|█████▎    | 265/500 [00:40<00:01, 208.87it/s, est. speed input: 815.12 toks/s, output: 11564.79 toks/s]\n",
      "Processed prompts:  59%|█████▉    | 294/500 [00:40<00:00, 229.87it/s, est. speed input: 860.70 toks/s, output: 12953.05 toks/s]\n",
      "Processed prompts:  68%|██████▊   | 340/500 [00:40<00:00, 284.92it/s, est. speed input: 925.59 toks/s, output: 15167.54 toks/s]\n",
      "Processed prompts:  81%|████████  | 403/500 [00:40<00:00, 375.73it/s, est. speed input: 1004.95 toks/s, output: 18212.15 toks/s]\n",
      "Processed prompts:  90%|█████████ | 452/500 [00:40<00:00, 399.83it/s, est. speed input: 1056.64 toks/s, output: 20565.61 toks/s]\n",
      "Processed prompts: 100%|██████████| 500/500 [00:41<00:00, 12.19it/s, est. speed input: 1096.39 toks/s, output: 22844.01 toks/s] \n",
      "\n",
      "✓ Evaluation data saved to: result_Base.parquet\n",
      "  Total samples: 500\n",
      "  Columns: ['prompt', 'response', 'reward', 'message_log', 'extra_env_info', 'sample_index', 'model_name', 'dataset_name', 'metric', 'num_tests_per_prompt', 'temperature', 'top_p', 'top_k']\n",
      "  File size: 0.61 MB\n",
      "\n",
      "============================================================\n",
      "model_name='Qwen2.5-1.5B' dataset_name='MATH-500'\n",
      "max_new_tokens=2048 temperature=0.0 top_p=1.0 top_k=-1\n",
      "\n",
      "metric='pass@1' num_tests_per_prompt=1\n",
      "\n",
      "score=0.0500 (25.0/500)\n",
      "============================================================\n",
      "\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Run evaluation for pre-RL base model\n",
    "!cd /root/verb-workspace/NeMo-RL && uv run python examples/run_eval.py \\\n",
    "    generation.model_name=Qwen/Qwen2.5-1.5B \\\n",
    "    data.dataset_name=HuggingFaceH4/MATH-500 \\\n",
    "    data.dataset_key=test \\\n",
    "    eval.save_path=result_Base.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712e2493",
   "metadata": {},
   "source": [
    "## Results\n",
    "Your results for the model trained with RL should have a significantly higher score than the score for the base model, indicating that reinforcement learning fine-tuning has substantially enhanced the model's mathematical reasoning capabilities.\n",
    "\n",
    "### Commonly Used Metrics Explained\n",
    "\n",
    "When evaluating reinforcement learning-trained language models, several key metrics are commonly used:\n",
    "\n",
    "- **pass@1**: The percentage of problems solved correctly on the first attempt. This measures how often the model generates a correct solution immediately without multiple tries.\n",
    "\n",
    "- **pass@k**: The percentage of problems for which at least one correct solution is found among k generated attempts. For example, pass@10 means generating 10 solutions and checking if any of them is correct. This metric accounts for the model's ability to eventually find the right answer when given multiple chances. This metric is usually calculated by taking n (a much larger number than k) samples and plugging the results into the unbiased estimator below:\n",
    "\n",
    "![pass@k unbiased estimator](passatk.png)\n",
    "\n",
    "This results in a more accurate result as evaluating pass@k by only doing k samples would result in a lot of variance in the result. More info about this can be found [here](https://github.com/huggingface/evaluate/blob/main/metrics/code_eval/code_eval.py#L198).\n",
    "\n",
    "- **maj@k (majority@k)**: The percentage of problems solved correctly when taking the majority vote among k generated solutions. This approach assumes that if the model generates multiple solutions, the most frequently occurring answer is likely to be correct.\n",
    "\n",
    "- **avg@n**: The average score across n evaluation runs or the average number of correct solutions out of n attempts. This provides a more stable estimate of model performance by reducing variance from single runs.\n",
    "These metrics help assess different aspects of model performance: pass@1 measures immediate accuracy, pass@k measures the model's potential when given multiple attempts, maj@k leverages consensus among multiple generations, and avg@n provides statistical reliability.\n",
    "\n",
    "We are evaluating the model with the `pass@1` metric in this example.\n",
    "\n",
    "\n",
    "Now let's take a closer look at the model outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b467fd-bf8c-4c00-9159-ddc60cbe81a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (2.3.1)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /opt/conda/lib/python3.10/site-packages (from pandas) (1.26.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install pandas pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b47810-35fe-485a-a8eb-e7a949409c86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /root/verb-workspace/mair-hub/rl-tutorial/kdd_labs/rl_lab\n",
      "\n",
      "Loading files:\n",
      "- RL results: ~/verb-workspace/NeMo-RL/result_RL.parquet\n",
      "- Base results: ~/verb-workspace/NeMo-RL/result_Base.parquet\n",
      "✓ Successfully loaded both files\n",
      "\n",
      "Dataset sizes:\n",
      "- RL model: 500 samples\n",
      "- Base model: 500 samples\n",
      "\n",
      "Accuracy:\n",
      "- RL model: 53.6%\n",
      "- Base model: 5.0%\n",
      "- Difference: 48.6%\n",
      "\n",
      "Common questions: 500 out of 500 RL prompts and 500 Base prompts\n",
      "✓ Found common questions!\n",
      "\n",
      "================================================================================\n",
      "COMPARISON EXAMPLE\n",
      "================================================================================\n",
      "QUESTION:\n",
      "<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "A polynomial with integer coefficients is of the form\n",
      "\\[2x^4 + a_3 x^3 + a_2 x^2 + a_1 x + 1 = 0.\\]Find the number of different possible rational roots of this polynomial.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "\n",
      "----------------------------------------\n",
      "RL MODEL RESPONSE:\n",
      "----------------------------------------\n",
      "To determine the number of different possible rational roots of the polynomial \\(2x^4 + a_3 x^3 + a_2 x^2 + a_1 x + 1 = 0\\), we can use the Rational Root Theorem. The Rational Root Theorem states that any potential rational root, expressed in its lowest terms \\(p/q\\), must have \\(p\\) as a factor of the constant term (the term without \\(x\\)) and \\(q\\) as a factor of the leading coefficient (the coe...\n",
      "✅ CORRECT\n",
      "\n",
      "----------------------------------------\n",
      "BASE MODEL RESPONSE:\n",
      "----------------------------------------\n",
      "afone\n",
      "afone\n",
      "afone\n",
      "afone\n",
      "afone\n",
      "afone\n",
      "afone\n",
      "afone\n",
      "afone\n",
      "afone\n",
      "afone\n",
      "afone\n",
      "afone\n",
      "afone\n",
      "afone\n",
      "afone\n",
      "afone\n",
      "afone\n",
      "afone\n",
      "afone\n",
      "afone\n",
      "afone\n",
      "afone\n",
      "afone\n",
      "afone\n",
      "afone\n",
      "afone\n",
      "afone\n",
      "afone\n",
      "afone\n",
      "afone\n",
      "afone\n",
      "afone\n",
      "afone\n",
      "afone\n",
      "afone\n",
      "afone\n",
      "afone\n",
      "afone\n",
      "afone\n",
      "afone\n",
      "afone\n",
      "afone\n",
      "afone\n",
      "afone\n",
      "afone\n",
      "afone\n",
      "afone\n",
      "afone\n",
      "afone\n",
      "afone\n",
      "afone\n",
      "afone\n",
      "afone\n",
      "afone\n",
      "afone\n",
      "afone\n",
      "afone\n",
      "afone\n",
      "afone\n",
      "afone\n",
      "afone\n",
      "afone\n",
      "afone\n",
      "afone\n",
      "afone\n",
      "afon...\n",
      "❌ WRONG\n",
      "\n",
      "----------------------------------------\n",
      "COMPARISON SUMMARY:\n",
      "----------------------------------------\n",
      "RL Model: ✅ Correct\n",
      "Base Model: ❌ Wrong\n",
      "🎯 RL model performed better!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Print current working directory\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "\n",
    "# Load both parquet files\n",
    "rl_path = \"~/verb-workspace/NeMo-RL/result_RL.parquet\"\n",
    "base_path = \"~/verb-workspace/NeMo-RL/result_Base.parquet\"\n",
    "\n",
    "print(f\"\\nLoading files:\")\n",
    "print(f\"- RL results: {rl_path}\")\n",
    "print(f\"- Base results: {base_path}\")\n",
    "\n",
    "try:\n",
    "    df_rl = pd.read_parquet(rl_path)\n",
    "    df_base = pd.read_parquet(base_path)\n",
    "    print(f\"✓ Successfully loaded both files\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading files: {e}\")\n",
    "    # Try alternative method\n",
    "    import pyarrow.parquet as pq\n",
    "    df_rl = pq.read_table(rl_path).to_pandas()\n",
    "    df_base = pq.read_table(base_path).to_pandas()\n",
    "\n",
    "# Basic statistics\n",
    "print(f\"\\nDataset sizes:\")\n",
    "print(f\"- RL model: {len(df_rl)} samples\")\n",
    "print(f\"- Base model: {len(df_base)} samples\")\n",
    "\n",
    "rl_accuracy = (df_rl['reward'] == 1).mean()\n",
    "base_accuracy = (df_base['reward'] == 1).mean()\n",
    "print(f\"\\nAccuracy:\")\n",
    "print(f\"- RL model: {rl_accuracy:.1%}\")\n",
    "print(f\"- Base model: {base_accuracy:.1%}\")\n",
    "print(f\"- Difference: {(rl_accuracy - base_accuracy):.1%}\")\n",
    "\n",
    "# Find questions that appear in both datasets\n",
    "# We'll match by prompt content\n",
    "rl_prompts = set(df_rl['prompt'].values)\n",
    "base_prompts = set(df_base['prompt'].values)\n",
    "common_prompts = rl_prompts.intersection(base_prompts)\n",
    "\n",
    "print(f\"\\nCommon questions: {len(common_prompts)} out of {len(rl_prompts)} RL prompts and {len(base_prompts)} Base prompts\")\n",
    "\n",
    "if len(common_prompts) == 0:\n",
    "    print(\"❌ No common questions found between datasets\")\n",
    "    # Show a few prompts from each to help debug\n",
    "    print(\"\\nFirst 3 RL prompts:\")\n",
    "    for i, prompt in enumerate(list(df_rl['prompt'].head(3))):\n",
    "        print(f\"{i+1}: {prompt[:100]}...\")\n",
    "    print(\"\\nFirst 3 Base prompts:\")\n",
    "    for i, prompt in enumerate(list(df_base['prompt'].head(3))):\n",
    "        print(f\"{i+1}: {prompt[:100]}...\")\n",
    "else:\n",
    "    print(\"✓ Found common questions!\")\n",
    "    \n",
    "    # Find questions where RL got correct and Base got wrong\n",
    "    rl_correct_wrong_samples = []\n",
    "    for prompt in common_prompts:\n",
    "        rl_sample = df_rl[df_rl['prompt'] == prompt].iloc[0]\n",
    "        base_sample = df_base[df_base['prompt'] == prompt].iloc[0]\n",
    "        \n",
    "        # Check if RL got correct (reward=1) and Base got wrong (reward=0)\n",
    "        if rl_sample['reward'] == 1 and base_sample['reward'] == 0:\n",
    "            rl_correct_wrong_samples.append((prompt, rl_sample, base_sample))\n",
    "    \n",
    "    print(f\"Found {len(rl_correct_wrong_samples)} questions where RL got correct and Base got wrong\")\n",
    "    \n",
    "    if len(rl_correct_wrong_samples) == 0:\n",
    "        print(\"❌ No samples found where RL performed better than Base\")\n",
    "        # Fall back to any common question\n",
    "        sample_prompt = list(common_prompts)[0]\n",
    "        rl_sample = df_rl[df_rl['prompt'] == sample_prompt].iloc[0]\n",
    "        base_sample = df_base[df_base['prompt'] == sample_prompt].iloc[0]\n",
    "        print(\"Showing first available sample instead:\")\n",
    "    else:\n",
    "        # Pick the first sample where RL performed better\n",
    "        sample_prompt, rl_sample, base_sample = rl_correct_wrong_samples[0]\n",
    "        print(\"✓ Showing sample where RL performed better than Base!\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"COMPARISON EXAMPLE\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Show the question\n",
    "    question = sample_prompt\n",
    "    if len(question) > 300:\n",
    "        question = question[:300] + \"...\"\n",
    "    print(f\"QUESTION:\\n{question}\")\n",
    "    \n",
    "    print(f\"\\n{'-'*40}\")\n",
    "    print(\"RL MODEL RESPONSE:\")\n",
    "    print(f\"{'-'*40}\")\n",
    "    rl_response = rl_sample['response']\n",
    "    if len(rl_response) > 400:\n",
    "        rl_response = rl_response[:400] + \"...\"\n",
    "    print(f\"{rl_response}\")\n",
    "    print(f\"✅ CORRECT\" if rl_sample['reward'] == 1 else \"❌ WRONG\")\n",
    "    \n",
    "    print(f\"\\n{'-'*40}\")\n",
    "    print(\"BASE MODEL RESPONSE:\")\n",
    "    print(f\"{'-'*40}\")\n",
    "    base_response = base_sample['response']\n",
    "    if len(base_response) > 400:\n",
    "        base_response = base_response[:400] + \"...\"\n",
    "    print(f\"{base_response}\")\n",
    "    print(f\"✅ CORRECT\" if base_sample['reward'] == 1 else \"❌ WRONG\")\n",
    "    \n",
    "    print(f\"\\n{'-'*40}\")\n",
    "    print(\"COMPARISON SUMMARY:\")\n",
    "    print(f\"{'-'*40}\")\n",
    "    print(f\"RL Model: {'✅ Correct' if rl_sample['reward'] == 1 else '❌ Wrong'}\")\n",
    "    print(f\"Base Model: {'✅ Correct' if base_sample['reward'] == 1 else '❌ Wrong'}\")\n",
    "    \n",
    "    if rl_sample['reward'] == base_sample['reward']:\n",
    "        print(\"🔄 Both models performed the same\")\n",
    "    elif rl_sample['reward'] == 1:\n",
    "        print(\"🎯 RL model performed better!\")\n",
    "    else:\n",
    "        print(\"📉 Base model performed better\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4517e9f",
   "metadata": {},
   "source": [
    "## Analysis of Results\n",
    "\n",
    "The evaluation demonstrates that the RL model successfully learned to generate coherent Chain-of-Thought reasoning steps and follow instructions properly, while the base model struggled to even produce well-formatted responses or follow the given instruction format. This shows how reinforcement learning can teach models not just what to output, but how to think through problems systematically, resulting in dramatically improved mathematical reasoning capabilities.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
